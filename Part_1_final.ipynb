{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tz68m52Ag1QE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hr-J22IJpGQ1"
   },
   "outputs": [],
   "source": [
    "def glove_dict_generation():\n",
    "    glove_dict = {}\n",
    "    with open('glove.6B.100d.txt', 'rb') as word_corpus:\n",
    "      for item in word_corpus:\n",
    "        item = item.decode().split()\n",
    "        word = item[0]\n",
    "        vect = np.array(item[1:]).astype(np.float)\n",
    "        if word not in glove_dict:\n",
    "          glove_dict[word] = vect\n",
    "        \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzshiLTZpIj8"
   },
   "outputs": [],
   "source": [
    "glove_dict=glove_dict_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKpi61tEZPm0"
   },
   "outputs": [],
   "source": [
    "def mean_embedding(x,dim,glove_dict):\n",
    "    return np.array(np.sum([glove_dict[w] for w in x if w in glove_dict] or [np.zeros(dim)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkN_0G0lpL-P"
   },
   "outputs": [],
   "source": [
    "def similarity(document,ending,glove_dict):\n",
    "    document_vector=mean_embedding(document,100,glove_dict)\n",
    "    ending_vector=mean_embedding(ending,100,glove_dict)\n",
    "    result = 1 - spatial.distance.cosine(document_vector, ending_vector)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4I15fBIG4EcR",
    "outputId": "3db36afd-27c7-4901-c70c-27b7485d9faa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\vigy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AI9tbODMg1QJ"
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "9ISVquocg1QL",
    "outputId": "f83eb0d0-e545-4c7a-b2e7-2f0c6ce85d87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InputStoryid</th>\n",
       "      <th>InputSentence1</th>\n",
       "      <th>InputSentence2</th>\n",
       "      <th>InputSentence3</th>\n",
       "      <th>InputSentence4</th>\n",
       "      <th>RandomFifthSentenceQuiz1</th>\n",
       "      <th>RandomFifthSentenceQuiz2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b929f263-1dcd-4a0b-b267-5d5ff2fe65bb</td>\n",
       "      <td>My friends all love to go to the club to dance.</td>\n",
       "      <td>They think it's a lot of fun and always invite.</td>\n",
       "      <td>I finally decided to tag along last Saturday.</td>\n",
       "      <td>I danced terribly and broke a friend's toe.</td>\n",
       "      <td>My friends decided to keep inviting me out as ...</td>\n",
       "      <td>The next weekend, I was asked to please stay h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7cbbc0af-bcce-4f56-871d-963f9bb6a99d</td>\n",
       "      <td>I tried going to the park the other day.</td>\n",
       "      <td>The weather seemed nice enough for a walk.</td>\n",
       "      <td>Within minutes of getting there I started snee...</td>\n",
       "      <td>My eyes were watery and it was hard to breathe.</td>\n",
       "      <td>My allergies were too bad and I had to go back...</td>\n",
       "      <td>It reminded me of how much I loved spring flow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           InputStoryid  \\\n",
       "0  b929f263-1dcd-4a0b-b267-5d5ff2fe65bb   \n",
       "1  7cbbc0af-bcce-4f56-871d-963f9bb6a99d   \n",
       "\n",
       "                                    InputSentence1  \\\n",
       "0  My friends all love to go to the club to dance.   \n",
       "1         I tried going to the park the other day.   \n",
       "\n",
       "                                    InputSentence2  \\\n",
       "0  They think it's a lot of fun and always invite.   \n",
       "1       The weather seemed nice enough for a walk.   \n",
       "\n",
       "                                      InputSentence3  \\\n",
       "0      I finally decided to tag along last Saturday.   \n",
       "1  Within minutes of getting there I started snee...   \n",
       "\n",
       "                                    InputSentence4  \\\n",
       "0      I danced terribly and broke a friend's toe.   \n",
       "1  My eyes were watery and it was hard to breathe.   \n",
       "\n",
       "                            RandomFifthSentenceQuiz1  \\\n",
       "0  My friends decided to keep inviting me out as ...   \n",
       "1  My allergies were too bad and I had to go back...   \n",
       "\n",
       "                            RandomFifthSentenceQuiz2  \n",
       "0  The next weekend, I was asked to please stay h...  \n",
       "1  It reminded me of how much I loved spring flow...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train = 'train.csv'\n",
    "path_val = 'dev.csv'\n",
    "path_test = 'test.csv'\n",
    "data_train = pd.read_csv(path_train)\n",
    "data_val = pd.read_csv(path_val)\n",
    "data_test =pd.read_csv(path_test)\n",
    "data_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3HJpm3gg1QP"
   },
   "source": [
    "\n",
    "### [Unk] word handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bq6DMC4wg1QQ"
   },
   "outputs": [],
   "source": [
    "# Used to add UNK to the training corpus\n",
    "def unknown_words(corpus):\n",
    "    for i in range(len(corpus)):\n",
    "        toss = np.random.binomial(size=1, n=1, p= 0.01)\n",
    "        if toss == 1:\n",
    "            corpus[i] = 'UNK'\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oB3X_c1zg1QT"
   },
   "outputs": [],
   "source": [
    "# Used to add UNK in validation and Testing corpus\n",
    "def add_unknown_words(corpus, unigram_count):\n",
    "    for i in range(len(corpus)):\n",
    "        if corpus[i] not in unigram_count:\n",
    "            corpus[i] = 'UNK'\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoPtTO12g1Qb"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpXfPPgwg1Qd"
   },
   "outputs": [],
   "source": [
    "def fetch_data(data, special_tokens, vocab_dict, training, validation, testing, lowercase):   # training, validation and testing are BOOLEAN values\n",
    "    story = ''\n",
    "    data_list = []\n",
    "    label = []\n",
    "    label_for_acc = []\n",
    "    sentiment1 = []\n",
    "    sentiment2= []\n",
    "    similarity_vector= []\n",
    "    \n",
    "    for item in data.iterrows():\n",
    "\n",
    "        if training == True:\n",
    "            \n",
    "            if special_tokens == None:\n",
    "                story += item[1][1] + ' ' + item[1][2] + ' ' + item[1][3] + ' ' + item[1][4] + ' ' + item[1][5] + ' ' + item[1][5]\n",
    "                story_body = item[1][1] + ' ' + item[1][2] + ' ' + item[1][3] + ' ' + item[1][4]\n",
    "                storyend1  = story_body + ' ' + item[1][5] \n",
    "                storyend2 = story_body + ' ' + item[1][6]\n",
    "                \n",
    "                if lowercase == True:\n",
    "                    story = story.lower()\n",
    "                    storyend1 = storyend1.lower()\n",
    "                    storyend2 = storyend2.lower()\n",
    "                \n",
    "                data_list.append(storyend1)\n",
    "                data_list.append(storyend2)\n",
    "                \n",
    "            else:\n",
    "                story_body = item[1][1] + ' ' + item[1][2] + ' ' + item[1][3] + ' ' + item[1][4]\n",
    "                storyend1  = story_body + ' ' + item[1][5] \n",
    "                storyend2 = story_body + ' ' + item[1][6]\n",
    "                sentiment1.append(sid.polarity_scores(story_body)['compound']-sid.polarity_scores(storyend1)['compound'])\n",
    "                sentiment1.append(sid.polarity_scores(story_body)['compound']-sid.polarity_scores(storyend2)['compound'])\n",
    "                similarity_vector.append(similarity(story_body,storyend1,glove_dict))\n",
    "                similarity_vector.append(similarity(story_body,storyend2,glove_dict))\n",
    "\n",
    "                if lowercase == True:\n",
    "                    storyend1 = storyend1.lower()\n",
    "                    storyend2 = storyend2.lower()\n",
    "\n",
    "                storyend1 = storyend1.split()\n",
    "                storyend2 = storyend2.split()\n",
    "\n",
    "                se1 = ' '\n",
    "                se2 = ' '\n",
    "\n",
    "                for i, word in enumerate(storyend1):\n",
    "                    if word in special_tokens:\n",
    "                        storyend1[i] = 'UNK'\n",
    "\n",
    "                for i, word in enumerate(storyend2):\n",
    "                    if word in special_tokens:\n",
    "                        storyend2[i] = 'UNK'\n",
    "\n",
    "                se1 = se1.join(storyend1)\n",
    "                se2 = se2.join(storyend2)\n",
    "\n",
    "                data_list.append(se1)\n",
    "                data_list.append(se2)\n",
    "\n",
    "            \n",
    "            if item[1][7] == 1:\n",
    "                label.append(1)\n",
    "                label.append(0)\n",
    "            else:\n",
    "                label.append(0)\n",
    "                label.append(1)\n",
    "        \n",
    "        if validation == True:\n",
    "            \n",
    "            story_body = item[1][1] + ' ' + item[1][2] + ' ' + item[1][3] + ' ' + item[1][4]\n",
    "            storyend1  = story_body + ' ' + item[1][5] \n",
    "            storyend2 = story_body + ' ' + item[1][6]\n",
    "            sentiment1.append(sid.polarity_scores(story_body)['compound']-sid.polarity_scores(storyend1)['compound'])\n",
    "            sentiment1.append(sid.polarity_scores(story_body)['compound']-sid.polarity_scores(storyend2)['compound'])\n",
    "            similarity_vector.append(similarity(story_body,storyend1,glove_dict))\n",
    "            similarity_vector.append(similarity(story_body,storyend2,glove_dict))\n",
    "\n",
    "            if lowercase == True:\n",
    "                storyend1 = storyend1.lower()\n",
    "                storyend2 = storyend2.lower()\n",
    "\n",
    "            \n",
    "            storyend1 = storyend1.split()\n",
    "            storyend2 = storyend2.split()\n",
    "\n",
    "            se1 = ' '\n",
    "            se2 = ' '\n",
    "            \n",
    "            for i in range(len(storyend1)):\n",
    "                if storyend1[i] not in vocab_dict:\n",
    "                    storyend1[i] = 'UNK'\n",
    "            \n",
    "            for i in range(len(storyend2)):\n",
    "                if storyend2[i] not in vocab_dict:\n",
    "                    storyend2[i] = 'UNK'\n",
    "\n",
    "            se1 = se1.join(storyend1)\n",
    "            se2 = se2.join(storyend2)\n",
    "\n",
    "            data_list.append(se1)\n",
    "            data_list.append(se2)\n",
    "            \n",
    "            \n",
    "            label_for_acc.append(item[1][7])\n",
    "            \n",
    "        \n",
    "            if item[1][7] == 1:\n",
    "                label.append(1)\n",
    "                label.append(0)\n",
    "            else:\n",
    "                label.append(0)\n",
    "                label.append(1)\n",
    "            \n",
    "        if testing == True:\n",
    "            \n",
    "            story_body = item[1][1] + ' ' + item[1][2] + ' ' + item[1][3] + ' ' + item[1][4]\n",
    "            storyend1  = story_body + ' ' + item[1][5] \n",
    "            storyend2 = story_body + ' ' + item[1][6]\n",
    "            sentiment1.append(sid.polarity_scores(story_body)['compound']-sid.polarity_scores(storyend1)['compound'])\n",
    "            sentiment1.append(sid.polarity_scores(story_body)['compound']-sid.polarity_scores(storyend2)['compound'])\n",
    "            similarity_vector.append(similarity(story_body,storyend1,glove_dict))\n",
    "            similarity_vector.append(similarity(story_body,storyend2,glove_dict))\n",
    "\n",
    "            if lowercase == True:\n",
    "                storyend1 = storyend1.lower()\n",
    "                storyend2 = storyend2.lower()\n",
    "\n",
    "            \n",
    "            storyend1 = storyend1.split()\n",
    "            storyend2 = storyend2.split()\n",
    "\n",
    "            se1 = ' '\n",
    "            se2 = ' '\n",
    "            \n",
    "            for i in range(len(storyend1)):\n",
    "                if storyend1[i] not in vocab_dict:\n",
    "                    storyend1[i] = 'UNK'\n",
    "            \n",
    "            for i in range(len(storyend2)):\n",
    "                if storyend2[i] not in vocab_dict:\n",
    "                    storyend2[i] = 'UNK'\n",
    "\n",
    "            se1 = se1.join(storyend1)\n",
    "            se2 = se2.join(storyend2)\n",
    "            \n",
    "            data_list.append(se1)\n",
    "            data_list.append(se2)\n",
    "            \n",
    "        \n",
    "\n",
    "    return story, data_list, label, label_for_acc,sentiment1,similarity_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sesoSawxg1Qg"
   },
   "source": [
    "### Unigram (Vocab count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPwZ5eebg1Qh"
   },
   "outputs": [],
   "source": [
    "# O(n) complexity\n",
    "def unigram(corpus):\n",
    "    unigram_count = {}\n",
    "    total_word_count = len(corpus)\n",
    "\n",
    "    for item in corpus: \n",
    "        if (item in unigram_count):   \n",
    "            unigram_count[item] += 1\n",
    "        else: \n",
    "            unigram_count[item] = 1\n",
    "    \n",
    "    return unigram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpwlmUYXg1Qk"
   },
   "source": [
    "### Find words that can be replaced with [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBpNiz3mg1Qk"
   },
   "outputs": [],
   "source": [
    "def replace_words_unk(vocab):\n",
    "    limit = 2\n",
    "    replacable_words = []\n",
    "    for word in vocab:\n",
    "        if vocab[word]<limit:\n",
    "            replacable_words.append(word)\n",
    "    \n",
    "    return replacable_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHwIOJaAg1Qn"
   },
   "source": [
    "### Label Assign in acceptable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0D3QeR2g1Qo"
   },
   "outputs": [],
   "source": [
    "def label_assign(ypred_val, prob_val):\n",
    "    label = []\n",
    "    for i in range(0, len(ypred_val), 2):\n",
    "        if ypred_val[i] < ypred_val[i+1]:\n",
    "            label.append(2)\n",
    "        \n",
    "        elif ypred_val[i] > ypred_val[i+1]:\n",
    "            label.append(1)\n",
    "        \n",
    "        elif ypred_val[i] == 0 and ypred_val[i+1] == 0:\n",
    "            if prob_val[i][ypred_val[i]]<prob_val[i+1][ypred_val[i+1]]:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(2)\n",
    "        \n",
    "        elif ypred_val[i] == 1 and ypred_val[i+1] == 1:\n",
    "            if prob_val[i][ypred_val[i]]<prob_val[i+1][ypred_val[i+1]]:\n",
    "                label.append(2)\n",
    "            else:\n",
    "                label.append(1)\n",
    "                \n",
    "       \n",
    "    return label        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "5tco-QLjg1Qr",
    "outputId": "390b546b-f076-40de-c0dc-0b9ed5342980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training data...\n",
      "# of training tokens: 73572\n",
      "Training vocab length: 9992\n",
      "Replace # of words with UNK: 5620\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    classifier = {'Logistic Regression':True, 'Multinomial NB': False, 'Random Forest':False, 'SVC':False, 'Gradient Boosting':False, 'XG':False} \n",
    "        \n",
    "    print('Loading Training data...')\n",
    "    train_corpus, _, _, _, _, _= fetch_data(data_train, special_tokens=None, vocab_dict=None, training=True, validation=False, testing=False, lowercase=True)\n",
    "    train_data_tokens = train_corpus.split()\n",
    "    print('# of training tokens:', len(train_data_tokens))\n",
    "    vocab_dict = unigram(train_data_tokens)\n",
    "    print('Training vocab length:',len(vocab_dict))\n",
    "    replace_words = replace_words_unk(vocab_dict)\n",
    "    print('Replace # of words with UNK:', len(replace_words))\n",
    "    _, train_data, train_label, _, train_sentiment1, train_similarity = fetch_data(data_train, special_tokens=replace_words, vocab_dict=None, training=True, validation=False, testing=False, lowercase=True)\n",
    "    print('# of training documents:', int(len(train_data)/2))\n",
    "    \n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print('Loading Validation Data...')\n",
    "    _, val_data, val_label, val_label_check,valid_sentiment1, valid_similarity = fetch_data(data_val, special_tokens=None, vocab_dict=vocab_dict, training=False, validation=True, testing=False, lowercase=True)\n",
    "    print('# of validation documents:',int(len(val_data)/2))\n",
    "    \n",
    "    # Training...\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,4), stop_words = None)\n",
    "    features_X_train = vectorizer.fit_transform(train_data)\n",
    "    print (type(features_X_train))\n",
    "    train_sentiment1 = np.asarray(train_sentiment1)\n",
    "    train_similarity = np.asarray(train_similarity)\n",
    "    s=train_sentiment1.shape[0]\n",
    "    train_sentiment1 = train_sentiment1.reshape(s, 1)\n",
    "    train_similarity = train_similarity.reshape(s, 1)\n",
    "    features_X_train=sparse.hstack((features_X_train,train_sentiment1,train_similarity))\n",
    "    #features_X_train=np.hstack((train_sentiment1,train_similarity))\n",
    "    word_types = vectorizer.get_feature_names()\n",
    "    \n",
    "    if classifier['Logistic Regression'] == True:\n",
    "        clf = LogisticRegression(random_state=0)\n",
    "    elif classifier['Multinomial NB'] == True:\n",
    "        clf = MultinomialNB(alpha = 1, class_prior=None, fit_prior=True)\n",
    "    elif classifier['Random Forest'] == True:\n",
    "        clf = RandomForestClassifier()\n",
    "    elif classifier['SVC'] == True:\n",
    "        clf = SVC(probability=True)\n",
    "    elif classifier['Gradient Boosting'] == True:\n",
    "        clf = GradientBoostingClassifier()\n",
    "    elif classifier['XG'] == True:\n",
    "        clf = XGBClassifier()\n",
    "        \n",
    "    \n",
    "    clf.fit(features_X_train, train_label)\n",
    "    \n",
    "    # Validation...\n",
    "    features_X_valid = vectorizer.transform(val_data)\n",
    "    valid_sentiment1 = np.asarray(valid_sentiment1)\n",
    "    valid_similarity = np.asarray(valid_similarity)\n",
    "    s=valid_sentiment1.shape[0]\n",
    "    valid_sentiment1 = valid_sentiment1.reshape((s, 1))\n",
    "    valid_similarity = valid_similarity.reshape((s, 1))\n",
    "    features_X_valid=sparse.hstack((features_X_valid,valid_sentiment1,valid_similarity))\n",
    "    #features_X_valid=np.hstack((valid_sentiment1,valid_similarity))\n",
    "\n",
    "    \n",
    "    ypred_val = clf.predict(features_X_valid)\n",
    "    prob_val = clf.predict_proba(features_X_valid)\n",
    "    prob_val = prob_val.tolist()\n",
    "    \n",
    "    y_result_val = label_assign(ypred_val, prob_val)\n",
    "    acc_score = accuracy_score(val_label_check, y_result_val)\n",
    "    print('Accuracy Score: %s'%(acc_score))\n",
    "    _, test_data, test_label, test_label_check,test_sentiment1, test_similarity = fetch_data(data_test, special_tokens=None, vocab_dict=vocab_dict, training=False, validation=False, testing=True, lowercase=True)\n",
    "    print('# of test documents:',int(len(test_data)/2))\n",
    "    \n",
    "    # Testing...\n",
    "    features_X_test = vectorizer.transform(test_data)\n",
    "    test_sentiment1 = np.asarray(test_sentiment1)\n",
    "    test_similarity = np.asarray(test_similarity)\n",
    "    s=test_sentiment1.shape[0]\n",
    "    test_sentiment1 = test_sentiment1.reshape((s, 1))\n",
    "    test_similarity = test_similarity.reshape((s, 1))\n",
    "    features_X_test=sparse.hstack((features_X_test,test_sentiment1,test_similarity))\n",
    "    #features_X_test=np.hstack((test_sentiment1,test_similarity))\n",
    "    \n",
    "    ypred_test = clf.predict(features_X_test)\n",
    "    prob_test = clf.predict_proba(features_X_test)\n",
    "    prob_test = prob_test.tolist()\n",
    "    \n",
    "    y_result_test = label_assign(ypred_test, prob_test)\n",
    "    \n",
    "    ids = data_test['InputStoryid']    \n",
    "    output = pd.DataFrame({'Id': ids,\n",
    "                            'Prediction': y_result_test})\n",
    "    output.set_index('Id')\n",
    "    path = 'result_partA.csv'\n",
    "    output.to_csv(path, index = False)\n",
    "    output.head(5)\n",
    "    \n",
    "    print('Test labels created in .csv format')\n",
    "    \n",
    "    \n",
    "    \n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BseiPFkXg1Qu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Part_1_abhi.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
